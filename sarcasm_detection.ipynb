{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "sarcasm_detection.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "487c9H7oFp1F",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "!unzip  elmo4irony.zip"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7WYBMj0ZH8-L",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "!pip install colored_traceback\n",
        "!pip install tensorboardX\n",
        "!pip install dataset"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "B2m2qaXIGzYD",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import os\n",
        "os.chdir(\"./elmo4irony\")"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nnfBMMQTHHNe",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import os\n",
        "\n",
        "from glob import glob\n",
        "\n",
        "import torch\n",
        "import colored_traceback\n",
        "import numpy as np\n",
        "from tqdm import tqdm\n",
        "from tensorboardX import SummaryWriter\n",
        "\n",
        "from src.corpus.corpus import ClassificationCorpus\n",
        "\n",
        "from src.utils.logger import Logger\n",
        "# from src.utils.ops import np_softmax\n",
        "\n",
        "# from src.train import Trainer\n",
        "# from src.optim.optim import OptimWithDecay\n",
        "from src import config\n",
        "\n",
        "# from src.models.classifier import Classifier\n",
        "\n",
        "# from src.layers.pooling import PoolingLayer\n",
        "\n",
        "from base_args import base_parser, CustomArgumentParser"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2DHwnM3LIwG8",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# 此处生成语料库\n",
        "corpus = ClassificationCorpus(config.corpora_dict, \"palek\",\n",
        "                              force_reload=False,\n",
        "                              train_data_proportion=0.8,\n",
        "                              dev_data_proportion=0.2,\n",
        "                              batch_size=32,\n",
        "                              lowercase=True,\n",
        "                              use_pos=False)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hW0FAO6nJMTZ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import torch\n",
        "import time\n",
        "from torch import nn\n",
        "from torch.autograd import Variable\n",
        "import numpy as np\n",
        "import re\n",
        "import random\n",
        "import json\n",
        "import os\n",
        "from torch.autograd import Variable\n",
        "import warnings\n",
        "warnings.simplefilter(\"ignore\", UserWarning)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "A8dZwbOEI6DG",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class RNNModel(nn.Module):\n",
        "  def __init__(self, n_token):\n",
        "      embed_size = config['embedding_size']\n",
        "      hidden_size = config['hidden_size']\n",
        "      n_layers = config['n_layers']\n",
        "      dropout = config['dropout']\n",
        "      bidirectional = config['bidirectional']\n",
        "      ffnn_layers = config['linear']\n",
        "      super(RNNModel, self).__init__()\n",
        "      self.embed = nn.Embedding(n_token, embed_size)\n",
        "      self.RNN = nn.LSTM(embed_size, hidden_size, n_layers,\n",
        "                          batch_first=True, dropout=dropout, bidirectional=bidirectional)\n",
        "      sentence_length = 58\n",
        "      batch_size = 32\n",
        "      curr_dim = hidden_size * (bidirectional + 1) * sentence_length\n",
        "      self.layers = [nn.Linear(curr_dim, ffnn_layers[0])]\n",
        "      for i, o in zip(ffnn_layers[0:-1], ffnn_layers[1:]):\n",
        "          self.layers.append(nn.Linear(i, o))\n",
        "      self.softmax = nn.Softmax(dim=1)\n",
        "\n",
        "  def forward(self, x):\n",
        "      # print(x.size())\n",
        "      embedding = self.embed(x)\n",
        "      output, hidden = self.RNN(embedding)\n",
        "      # print(output.size())\n",
        "      output = output.reshape(output.size(0), -1).cuda()\n",
        "      # print(output.size())\n",
        "      for layer in self.layers:\n",
        "          output = layer.cuda()(output)\n",
        "          # output = nn.ReLU().cuda()(output)\n",
        "          # print(output.size())\n",
        "\n",
        "      output = self.softmax.cuda()(output)\n",
        "      \n",
        "      return output.view(-1, 2).cuda()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZzO1zVEuJVUx",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6jRvjKbYwi_P",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def train(model, corpus):\n",
        "    model.cuda()\n",
        "    model.train()\n",
        "    start = time.time()\n",
        "    batch_id = 0\n",
        "    criterion = nn.CrossEntropyLoss()\n",
        "    window_size = config['window_size']\n",
        "    batch_size = config[\"batch_size\"]\n",
        "    window_step = config[\"window_step_size\"]\n",
        "    optimizer = torch.optim.Adam(model.parameters())\n",
        "    data_generator = corpus.train_batches\n",
        "    total_loss = 0\n",
        "    print_period = 200\n",
        "    for batchIter in data_generator.gen():\n",
        "        x = batchIter[\"sequences\"]\n",
        "        y = batchIter[\"labels\"]\n",
        "        x = torch.LongTensor(x)\n",
        "        y = torch.LongTensor(y)\n",
        "        x, y = Variable(x.cuda()), Variable(y.cuda())\n",
        "        output = model(x)\n",
        "        # print(output)\n",
        "        output = output.view(-1, 2)\n",
        "        y = y.view(-1)\n",
        "        # print(len(output))\n",
        "        # print(len(output[0]))\n",
        "        # print(y)\n",
        "\n",
        "        loss = criterion(output, y)\n",
        "        optimizer.zero_grad()\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "        total_loss += loss.data\n",
        "        batch_id += 1\n",
        "        if batch_id % print_period == 0:\n",
        "          print('batchid:{} time:{}s loss:{}'.format(batch_id, time.time()-start, total_loss/print_period))\n",
        "          total_loss = 0\n",
        "          start = time.time()\n",
        "\n",
        "def test(model, corpus):\n",
        "    model.cuda()\n",
        "    model.eval()\n",
        "    start = time.time()\n",
        "    window_size = config['window_size']\n",
        "    test_batch_size = config[\"test_batch_size\"]\n",
        "    window_step = config[\"window_step_size\"]\n",
        "    test_data = corpus.test_batches\n",
        "    criterion = nn.CrossEntropyLoss()\n",
        "    total_loss = 0\n",
        "    true_pos = 0\n",
        "    false_pos = 0\n",
        "    true_neg = 0\n",
        "    false_neg = 0\n",
        "    batch_counter = 0\n",
        "    mark_counter = 0\n",
        "    for batchIter in test_data.gen():\n",
        "        x = batchIter[\"sequences\"]\n",
        "        y = batchIter[\"labels\"]\n",
        "        x = torch.LongTensor(x)\n",
        "        y = torch.LongTensor(y)\n",
        "\n",
        "        x, y = Variable(x.cuda()), Variable(y.cuda())\n",
        "        prediction = model(x)\n",
        "        loss = criterion(prediction, y)\n",
        "        total_loss += loss\n",
        "        prediction[prediction[:, 1] > 0.5] = 1\n",
        "        prediction[prediction[:, 1] <= 0.5] = 0\n",
        "        prediction = prediction[:, 1].bool()\n",
        "        # print(prediction)\n",
        "        y= y.bool()\n",
        "        true_pos += (y * prediction).sum()\n",
        "        false_neg += (y * (~prediction)).sum()\n",
        "        false_pos += ((~y) * prediction).sum()\n",
        "        true_neg += ((~y) * (~prediction)).sum()\n",
        "        # print(true_pos)\n",
        "\n",
        "        batch_counter += 1\n",
        "    return print_info(true_pos, false_pos, true_neg, false_neg, start, total_loss, batch_counter)\n",
        "    print(total_loss/batch_counter)\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MAaEUXXsJVmX",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def print_info(true_pos, false_pos, true_neg, false_neg, since, total_loss, batch_counter):\n",
        "    print('-' * 15)\n",
        "    print('TP: %d\\nTN: %d\\nFP: %d\\nFN: %d\\nmark_counter: \\n' %\n",
        "          (true_pos, true_neg, false_pos, false_neg))\n",
        "    print('-' * 15)\n",
        "    print('time: %.3f s\\nloss: %.4f\\nprecision: %.4f\\nrecall: %.4f\\nf score: %.4f\\naccuracy: %.4f'\n",
        "          % (\n",
        "              time.time() - since,\n",
        "              total_loss / batch_counter,\n",
        "              true_pos / (true_pos + false_pos + 1e-6),\n",
        "              true_pos / (true_pos + false_neg + 1e-6),\n",
        "              2 * true_pos / (2 * true_pos + false_pos + false_neg + 1e-6),\n",
        "              (true_pos + true_neg) / (true_neg + true_pos + false_neg + false_pos)\n",
        "          ))\n",
        "    print('-' * 15)\n",
        "    return 2 * true_pos / (2 * true_pos + false_pos + false_neg + 1e-6)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "R_FiesJypLix",
        "colab_type": "code",
        "outputId": "12725ba1-e520-4fa1-d90f-d826a250c9fb",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "\n",
        "config = {\n",
        "    \"embedding_size\": 100,\n",
        "    \"hidden_size\": 100,\n",
        "    \"n_layers\": 3,\n",
        "    \"dropout\": 0.2,\n",
        "    \"bidirectional\": True,\n",
        "    \"linear\": [100, 100, 2],\n",
        "    \"learning_rate\": 0.01,\n",
        "    'init_range': 0.1,\n",
        "    'test_batch_size': 16,\n",
        "    \"batch_size\": 32,\n",
        "    \"window_size\": 28,\n",
        "    \"window_step_size\": 25,\n",
        "}\n",
        "model = RNNModel(len(corpus.lang.token2id))\n",
        "nEpochs = 100\n",
        "for epoch in range(nEpochs):\n",
        "    train(model, corpus)\n",
        "    test(model, corpus)\n"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "batchid:200 time:6.697207689285278s loss:0.6424132585525513\n",
            "batchid:400 time:6.591456174850464s loss:0.5855687856674194\n",
            "batchid:600 time:6.615060091018677s loss:0.5685141682624817\n",
            "batchid:800 time:6.602676868438721s loss:0.5823965072631836\n",
            "batchid:1000 time:6.592909812927246s loss:0.5524418354034424\n",
            "---------------\n",
            "TP: 5813\n",
            "TN: 3708\n",
            "FP: 2552\n",
            "FN: 594\n",
            "mark_counter: \n",
            "\n",
            "---------------\n",
            "time: 7.754 s\n",
            "loss: 0.5503\n",
            "precision: 0.6949\n",
            "recall: 0.9073\n",
            "f score: 0.7870\n",
            "accuracy: 0.0000\n",
            "---------------\n",
            "batchid:200 time:6.661293268203735s loss:0.5486118793487549\n",
            "batchid:400 time:6.606712579727173s loss:0.5261557102203369\n",
            "batchid:600 time:6.597862958908081s loss:0.5142425894737244\n",
            "batchid:800 time:6.573846817016602s loss:0.5152010917663574\n",
            "batchid:1000 time:6.6177122592926025s loss:0.4951779544353485\n",
            "---------------\n",
            "TP: 5494\n",
            "TN: 4632\n",
            "FP: 1628\n",
            "FN: 913\n",
            "mark_counter: \n",
            "\n",
            "---------------\n",
            "time: 7.747 s\n",
            "loss: 0.5061\n",
            "precision: 0.7714\n",
            "recall: 0.8575\n",
            "f score: 0.8122\n",
            "accuracy: 0.0000\n",
            "---------------\n",
            "batchid:200 time:6.660387992858887s loss:0.4840982258319855\n",
            "batchid:400 time:6.641907215118408s loss:0.47352099418640137\n",
            "batchid:600 time:6.58937931060791s loss:0.4689478576183319\n",
            "batchid:800 time:6.621999979019165s loss:0.4673102796077728\n",
            "batchid:1000 time:6.587573766708374s loss:0.46962401270866394\n",
            "---------------\n",
            "TP: 5676\n",
            "TN: 4568\n",
            "FP: 1692\n",
            "FN: 731\n",
            "mark_counter: \n",
            "\n",
            "---------------\n",
            "time: 7.775 s\n",
            "loss: 0.4981\n",
            "precision: 0.7704\n",
            "recall: 0.8859\n",
            "f score: 0.8241\n",
            "accuracy: 0.0000\n",
            "---------------\n",
            "batchid:200 time:6.662988185882568s loss:0.4573299288749695\n",
            "batchid:400 time:6.5846991539001465s loss:0.44647151231765747\n",
            "batchid:600 time:6.5606279373168945s loss:0.4399123191833496\n",
            "batchid:800 time:6.575533866882324s loss:0.44509655237197876\n",
            "batchid:1000 time:6.544634580612183s loss:0.4383198320865631\n",
            "---------------\n",
            "TP: 5672\n",
            "TN: 4666\n",
            "FP: 1594\n",
            "FN: 735\n",
            "mark_counter: \n",
            "\n",
            "---------------\n",
            "time: 7.719 s\n",
            "loss: 0.4891\n",
            "precision: 0.7806\n",
            "recall: 0.8853\n",
            "f score: 0.8297\n",
            "accuracy: 0.0000\n",
            "---------------\n",
            "batchid:200 time:6.7098588943481445s loss:0.43419376015663147\n",
            "batchid:400 time:6.708606243133545s loss:0.4256616234779358\n",
            "batchid:600 time:6.5921854972839355s loss:0.4197905361652374\n",
            "batchid:800 time:6.558140754699707s loss:0.4247203469276428\n",
            "batchid:1000 time:6.587473392486572s loss:0.41610556840896606\n",
            "---------------\n",
            "TP: 5478\n",
            "TN: 4806\n",
            "FP: 1454\n",
            "FN: 929\n",
            "mark_counter: \n",
            "\n",
            "---------------\n",
            "time: 7.674 s\n",
            "loss: 0.4941\n",
            "precision: 0.7902\n",
            "recall: 0.8550\n",
            "f score: 0.8214\n",
            "accuracy: 0.0000\n",
            "---------------\n",
            "batchid:200 time:6.626614809036255s loss:0.4181262254714966\n",
            "batchid:400 time:6.492161273956299s loss:0.408243328332901\n",
            "batchid:600 time:6.531876802444458s loss:0.4023537337779999\n",
            "batchid:800 time:6.518589735031128s loss:0.4049699008464813\n",
            "batchid:1000 time:6.5196006298065186s loss:0.3953994810581207\n",
            "---------------\n",
            "TP: 5546\n",
            "TN: 4812\n",
            "FP: 1448\n",
            "FN: 861\n",
            "mark_counter: \n",
            "\n",
            "---------------\n",
            "time: 7.627 s\n",
            "loss: 0.4876\n",
            "precision: 0.7930\n",
            "recall: 0.8656\n",
            "f score: 0.8277\n",
            "accuracy: 0.0000\n",
            "---------------\n",
            "batchid:200 time:6.567716121673584s loss:0.40259450674057007\n",
            "batchid:400 time:6.4682862758636475s loss:0.3898354768753052\n",
            "batchid:600 time:6.516691446304321s loss:0.3919597566127777\n",
            "batchid:800 time:6.544532537460327s loss:0.3904687166213989\n",
            "batchid:1000 time:6.515361785888672s loss:0.38606345653533936\n",
            "---------------\n",
            "TP: 5556\n",
            "TN: 4819\n",
            "FP: 1441\n",
            "FN: 851\n",
            "mark_counter: \n",
            "\n",
            "---------------\n",
            "time: 7.632 s\n",
            "loss: 0.4896\n",
            "precision: 0.7941\n",
            "recall: 0.8672\n",
            "f score: 0.8290\n",
            "accuracy: 0.0000\n",
            "---------------\n",
            "batchid:200 time:6.579904794692993s loss:0.3907508850097656\n",
            "batchid:400 time:6.584604263305664s loss:0.37741005420684814\n",
            "batchid:600 time:6.583194971084595s loss:0.3818138837814331\n",
            "batchid:800 time:6.596705913543701s loss:0.3843193054199219\n",
            "batchid:1000 time:6.5343005657196045s loss:0.375955194234848\n",
            "---------------\n",
            "TP: 5798\n",
            "TN: 4469\n",
            "FP: 1791\n",
            "FN: 609\n",
            "mark_counter: \n",
            "\n",
            "---------------\n",
            "time: 7.701 s\n",
            "loss: 0.4992\n",
            "precision: 0.7640\n",
            "recall: 0.9049\n",
            "f score: 0.8285\n",
            "accuracy: 0.0000\n",
            "---------------\n",
            "batchid:200 time:6.597360134124756s loss:0.3881082534790039\n",
            "batchid:400 time:6.459144353866577s loss:0.3702569603919983\n",
            "batchid:600 time:6.475565433502197s loss:0.3744378685951233\n",
            "batchid:800 time:6.53761887550354s loss:0.3749896287918091\n",
            "batchid:1000 time:6.540448904037476s loss:0.36932870745658875\n",
            "---------------\n",
            "TP: 5661\n",
            "TN: 4719\n",
            "FP: 1541\n",
            "FN: 746\n",
            "mark_counter: \n",
            "\n",
            "---------------\n",
            "time: 7.620 s\n",
            "loss: 0.4868\n",
            "precision: 0.7860\n",
            "recall: 0.8836\n",
            "f score: 0.8319\n",
            "accuracy: 0.0000\n",
            "---------------\n",
            "batchid:200 time:6.609412908554077s loss:0.3756057620048523\n",
            "batchid:400 time:6.480246543884277s loss:0.36604517698287964\n",
            "batchid:600 time:6.51645565032959s loss:0.3692033588886261\n",
            "batchid:800 time:6.552184343338013s loss:0.37278929352760315\n",
            "batchid:1000 time:6.488002777099609s loss:0.3696150481700897\n",
            "---------------\n",
            "TP: 5526\n",
            "TN: 4784\n",
            "FP: 1476\n",
            "FN: 881\n",
            "mark_counter: \n",
            "\n",
            "---------------\n",
            "time: 7.607 s\n",
            "loss: 0.4947\n",
            "precision: 0.7892\n",
            "recall: 0.8625\n",
            "f score: 0.8242\n",
            "accuracy: 0.0000\n",
            "---------------\n",
            "batchid:200 time:6.697309732437134s loss:0.3727336525917053\n",
            "batchid:400 time:6.489665985107422s loss:0.35919642448425293\n",
            "batchid:600 time:6.489430904388428s loss:0.3625710606575012\n",
            "batchid:800 time:6.478432893753052s loss:0.3729139268398285\n",
            "batchid:1000 time:6.515212774276733s loss:0.3657824695110321\n",
            "---------------\n",
            "TP: 5487\n",
            "TN: 4859\n",
            "FP: 1401\n",
            "FN: 920\n",
            "mark_counter: \n",
            "\n",
            "---------------\n",
            "time: 7.563 s\n",
            "loss: 0.4925\n",
            "precision: 0.7966\n",
            "recall: 0.8564\n",
            "f score: 0.8254\n",
            "accuracy: 0.0000\n",
            "---------------\n",
            "batchid:200 time:6.531413555145264s loss:0.3657248616218567\n",
            "batchid:400 time:6.516167879104614s loss:0.3566409945487976\n",
            "batchid:600 time:6.5666892528533936s loss:0.35760679841041565\n",
            "batchid:800 time:6.547609567642212s loss:0.36239612102508545\n",
            "batchid:1000 time:6.522320985794067s loss:0.35817262530326843\n",
            "---------------\n",
            "TP: 5771\n",
            "TN: 4398\n",
            "FP: 1862\n",
            "FN: 636\n",
            "mark_counter: \n",
            "\n",
            "---------------\n",
            "time: 7.649 s\n",
            "loss: 0.5054\n",
            "precision: 0.7561\n",
            "recall: 0.9007\n",
            "f score: 0.8221\n",
            "accuracy: 0.0000\n",
            "---------------\n",
            "batchid:200 time:6.569281339645386s loss:0.35805606842041016\n",
            "batchid:400 time:6.57555079460144s loss:0.3517726957798004\n",
            "batchid:600 time:6.49309229850769s loss:0.35261282324790955\n",
            "batchid:800 time:6.491726875305176s loss:0.356941282749176\n",
            "batchid:1000 time:6.476324796676636s loss:0.35509082674980164\n",
            "---------------\n",
            "TP: 5693\n",
            "TN: 4650\n",
            "FP: 1610\n",
            "FN: 714\n",
            "mark_counter: \n",
            "\n",
            "---------------\n",
            "time: 7.655 s\n",
            "loss: 0.4931\n",
            "precision: 0.7795\n",
            "recall: 0.8886\n",
            "f score: 0.8305\n",
            "accuracy: 0.0000\n",
            "---------------\n",
            "batchid:200 time:6.592808723449707s loss:0.35319390892982483\n",
            "batchid:400 time:6.537453651428223s loss:0.347088485956192\n",
            "batchid:600 time:6.492466926574707s loss:0.35105857253074646\n",
            "batchid:800 time:6.489640951156616s loss:0.35370802879333496\n",
            "batchid:1000 time:6.494173049926758s loss:0.34970125555992126\n",
            "---------------\n",
            "TP: 5702\n",
            "TN: 4457\n",
            "FP: 1803\n",
            "FN: 705\n",
            "mark_counter: \n",
            "\n",
            "---------------\n",
            "time: 7.566 s\n",
            "loss: 0.5077\n",
            "precision: 0.7598\n",
            "recall: 0.8900\n",
            "f score: 0.8197\n",
            "accuracy: 0.0000\n",
            "---------------\n",
            "batchid:200 time:6.530155181884766s loss:0.35076993703842163\n",
            "batchid:400 time:6.485509395599365s loss:0.3432846665382385\n",
            "batchid:600 time:6.52448582649231s loss:0.34782087802886963\n",
            "batchid:800 time:6.483129024505615s loss:0.35276365280151367\n",
            "batchid:1000 time:6.4782185554504395s loss:0.34959423542022705\n",
            "---------------\n",
            "TP: 5680\n",
            "TN: 4589\n",
            "FP: 1671\n",
            "FN: 727\n",
            "mark_counter: \n",
            "\n",
            "---------------\n",
            "time: 7.585 s\n",
            "loss: 0.4983\n",
            "precision: 0.7727\n",
            "recall: 0.8865\n",
            "f score: 0.8257\n",
            "accuracy: 0.0000\n",
            "---------------\n",
            "batchid:200 time:6.617651700973511s loss:0.3487444519996643\n",
            "batchid:400 time:6.443367958068848s loss:0.3444342315196991\n",
            "batchid:600 time:6.464853525161743s loss:0.3456117808818817\n",
            "batchid:800 time:6.502075433731079s loss:0.3502938747406006\n",
            "batchid:1000 time:6.488984107971191s loss:0.34928885102272034\n",
            "---------------\n",
            "TP: 5551\n",
            "TN: 4745\n",
            "FP: 1515\n",
            "FN: 856\n",
            "mark_counter: \n",
            "\n",
            "---------------\n",
            "time: 7.612 s\n",
            "loss: 0.4968\n",
            "precision: 0.7856\n",
            "recall: 0.8664\n",
            "f score: 0.8240\n",
            "accuracy: 0.0000\n",
            "---------------\n",
            "batchid:200 time:6.544801712036133s loss:0.34686213731765747\n",
            "batchid:400 time:6.511877059936523s loss:0.34233373403549194\n",
            "batchid:600 time:6.486236333847046s loss:0.3443847596645355\n",
            "batchid:800 time:6.480862379074097s loss:0.34756991267204285\n",
            "batchid:1000 time:6.4596123695373535s loss:0.34454283118247986\n",
            "---------------\n",
            "TP: 5592\n",
            "TN: 4686\n",
            "FP: 1574\n",
            "FN: 815\n",
            "mark_counter: \n",
            "\n",
            "---------------\n",
            "time: 7.587 s\n",
            "loss: 0.5000\n",
            "precision: 0.7804\n",
            "recall: 0.8728\n",
            "f score: 0.8240\n",
            "accuracy: 0.0000\n",
            "---------------\n",
            "batchid:200 time:6.523411750793457s loss:0.34456542134284973\n",
            "batchid:400 time:6.434625148773193s loss:0.3386181890964508\n",
            "batchid:600 time:6.440582990646362s loss:0.3414667844772339\n",
            "batchid:800 time:6.4724345207214355s loss:0.34553003311157227\n",
            "batchid:1000 time:6.4500412940979s loss:0.34086859226226807\n",
            "---------------\n",
            "TP: 5582\n",
            "TN: 4578\n",
            "FP: 1682\n",
            "FN: 825\n",
            "mark_counter: \n",
            "\n",
            "---------------\n",
            "time: 7.589 s\n",
            "loss: 0.5070\n",
            "precision: 0.7684\n",
            "recall: 0.8712\n",
            "f score: 0.8166\n",
            "accuracy: 0.0000\n",
            "---------------\n",
            "batchid:200 time:6.552501440048218s loss:0.34273478388786316\n",
            "batchid:400 time:6.459446668624878s loss:0.33646368980407715\n",
            "batchid:600 time:6.614675998687744s loss:0.33819255232810974\n",
            "batchid:800 time:6.5483081340789795s loss:0.3428124487400055\n",
            "batchid:1000 time:6.4869468212127686s loss:0.34041234850883484\n",
            "---------------\n",
            "TP: 5575\n",
            "TN: 4622\n",
            "FP: 1638\n",
            "FN: 832\n",
            "mark_counter: \n",
            "\n",
            "---------------\n",
            "time: 7.559 s\n",
            "loss: 0.5032\n",
            "precision: 0.7729\n",
            "recall: 0.8701\n",
            "f score: 0.8186\n",
            "accuracy: 0.0000\n",
            "---------------\n",
            "batchid:200 time:6.564405679702759s loss:0.34138813614845276\n",
            "batchid:400 time:6.443848133087158s loss:0.3348929286003113\n",
            "batchid:600 time:6.427478790283203s loss:0.3370451331138611\n",
            "batchid:800 time:6.40716290473938s loss:0.34234848618507385\n",
            "batchid:1000 time:6.436018943786621s loss:0.3410367965698242\n",
            "---------------\n",
            "TP: 5245\n",
            "TN: 4916\n",
            "FP: 1344\n",
            "FN: 1162\n",
            "mark_counter: \n",
            "\n",
            "---------------\n",
            "time: 7.542 s\n",
            "loss: 0.5075\n",
            "precision: 0.7960\n",
            "recall: 0.8186\n",
            "f score: 0.8072\n",
            "accuracy: 0.0000\n",
            "---------------\n",
            "batchid:200 time:6.5055458545684814s loss:0.3389541804790497\n",
            "batchid:400 time:6.467984914779663s loss:0.33259421586990356\n",
            "batchid:600 time:6.448435306549072s loss:0.33569446206092834\n",
            "batchid:800 time:6.52998685836792s loss:0.3385573625564575\n",
            "batchid:1000 time:6.532986164093018s loss:0.33647701144218445\n",
            "---------------\n",
            "TP: 5696\n",
            "TN: 4546\n",
            "FP: 1714\n",
            "FN: 711\n",
            "mark_counter: \n",
            "\n",
            "---------------\n",
            "time: 7.549 s\n",
            "loss: 0.5001\n",
            "precision: 0.7687\n",
            "recall: 0.8890\n",
            "f score: 0.8245\n",
            "accuracy: 0.0000\n",
            "---------------\n",
            "batchid:200 time:6.4850122928619385s loss:0.33525460958480835\n",
            "batchid:400 time:6.466226100921631s loss:0.3338176906108856\n",
            "batchid:600 time:6.467315196990967s loss:0.3340997099876404\n",
            "batchid:800 time:6.441705226898193s loss:0.3375680148601532\n",
            "batchid:1000 time:6.47999906539917s loss:0.3347730040550232\n",
            "---------------\n",
            "TP: 5675\n",
            "TN: 4362\n",
            "FP: 1898\n",
            "FN: 732\n",
            "mark_counter: \n",
            "\n",
            "---------------\n",
            "time: 7.527 s\n",
            "loss: 0.5177\n",
            "precision: 0.7494\n",
            "recall: 0.8857\n",
            "f score: 0.8119\n",
            "accuracy: 0.0000\n",
            "---------------\n",
            "batchid:200 time:6.484424591064453s loss:0.33685848116874695\n",
            "batchid:400 time:6.412137985229492s loss:0.3338211476802826\n",
            "batchid:600 time:6.502801418304443s loss:0.33527159690856934\n",
            "batchid:800 time:6.474942684173584s loss:0.3362593352794647\n",
            "batchid:1000 time:6.357619524002075s loss:0.3349246382713318\n",
            "---------------\n",
            "TP: 5577\n",
            "TN: 4663\n",
            "FP: 1597\n",
            "FN: 830\n",
            "mark_counter: \n",
            "\n",
            "---------------\n",
            "time: 7.539 s\n",
            "loss: 0.5008\n",
            "precision: 0.7774\n",
            "recall: 0.8705\n",
            "f score: 0.8213\n",
            "accuracy: 0.0000\n",
            "---------------\n",
            "batchid:200 time:6.516000270843506s loss:0.33335646986961365\n",
            "batchid:400 time:6.4007346630096436s loss:0.3327163755893707\n",
            "batchid:600 time:6.428133487701416s loss:0.33370646834373474\n",
            "batchid:800 time:6.414201021194458s loss:0.3355252146720886\n",
            "batchid:1000 time:6.405911207199097s loss:0.33177077770233154\n",
            "---------------\n",
            "TP: 5361\n",
            "TN: 4904\n",
            "FP: 1356\n",
            "FN: 1046\n",
            "mark_counter: \n",
            "\n",
            "---------------\n",
            "time: 7.499 s\n",
            "loss: 0.4999\n",
            "precision: 0.7981\n",
            "recall: 0.8367\n",
            "f score: 0.8170\n",
            "accuracy: 0.0000\n",
            "---------------\n",
            "batchid:200 time:6.491255521774292s loss:0.33410316705703735\n",
            "batchid:400 time:6.435743808746338s loss:0.33166465163230896\n",
            "batchid:600 time:6.422254800796509s loss:0.3329533338546753\n",
            "batchid:800 time:6.457618713378906s loss:0.334994912147522\n",
            "batchid:1000 time:6.39043664932251s loss:0.3316612243652344\n",
            "---------------\n",
            "TP: 5418\n",
            "TN: 4746\n",
            "FP: 1514\n",
            "FN: 989\n",
            "mark_counter: \n",
            "\n",
            "---------------\n",
            "time: 7.520 s\n",
            "loss: 0.5076\n",
            "precision: 0.7816\n",
            "recall: 0.8456\n",
            "f score: 0.8124\n",
            "accuracy: 0.0000\n",
            "---------------\n",
            "batchid:200 time:6.588130474090576s loss:0.33448657393455505\n",
            "batchid:400 time:6.5410261154174805s loss:0.33337777853012085\n",
            "batchid:600 time:6.387606859207153s loss:0.3316896855831146\n",
            "batchid:800 time:6.528143644332886s loss:0.33543097972869873\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-23-38771a75f195>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     17\u001b[0m \u001b[0mnEpochs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m100\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     18\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mepoch\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnEpochs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 19\u001b[0;31m     \u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcorpus\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     20\u001b[0m     \u001b[0mtest\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcorpus\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-21-c9eecd91792c>\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(model, corpus)\u001b[0m\n\u001b[1;32m     29\u001b[0m         \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mzero_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     30\u001b[0m         \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 31\u001b[0;31m         \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     32\u001b[0m         \u001b[0mtotal_loss\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     33\u001b[0m         \u001b[0mbatch_id\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/torch/optim/adam.py\u001b[0m in \u001b[0;36mstep\u001b[0;34m(self, closure)\u001b[0m\n\u001b[1;32m     93\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     94\u001b[0m                 \u001b[0;31m# Decay the first and second moment running average coefficient\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 95\u001b[0;31m                 \u001b[0mexp_avg\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmul_\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbeta1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0madd_\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mbeta1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgrad\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     96\u001b[0m                 \u001b[0mexp_avg_sq\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmul_\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbeta2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0maddcmul_\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mbeta2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgrad\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgrad\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     97\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0mamsgrad\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XswAudthJVxh",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "batches = next(corpus.test_batches.gen())\n",
        "x = batches['sequences']\n",
        "y = batches['labels']\n",
        "x = torch.LongTensor(x)\n",
        "y = torch.LongTensor(y)\n",
        "x, y = Variable(x.cuda()), Variable(y.cuda())"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "l7ZeREFTkAMk",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "prediction = model(x)\n",
        "prediction[prediction[:, 1] > 0.5] = 1\n",
        "prediction[prediction[:, 1] <= 0.5] = 0\n",
        "\n",
        "prediction = prediction[:, 1].bool()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bB05qrVf9KjB",
        "colab_type": "code",
        "outputId": "21c37e6d-a961-4019-add6-b16124548163",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 297
        }
      },
      "source": [
        "for i in x: print(len(i))"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "58\n",
            "58\n",
            "58\n",
            "58\n",
            "58\n",
            "58\n",
            "58\n",
            "58\n",
            "58\n",
            "58\n",
            "58\n",
            "58\n",
            "58\n",
            "58\n",
            "58\n",
            "58\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}